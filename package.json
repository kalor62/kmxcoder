{
  "name": "kmxcoder",
  "displayName": "kmxcoder",
  "description": "KMX Coder",
  "version": "0.0.1",
  "engines": {
    "vscode": "^1.77.0"
  },
  "categories": [
    "Other"
  ],
  "repository": {
    "type": "git",
    "url": "git://github.com/kalor62/kmxcoder.git"
  },
  "activationEvents": [],
  "main": "./out/extension.js",
  "contributes": {
    "configuration": {
      "title": "KMX Coder",
      "properties": {
        "kmxcoder.openaiApiKey": {
          "type": "string",
          "default": "",
          "scope": "window",
          "markdownDescription": "The OpenAI API Key"
        },
        "kmxcoder.openaiEngineModel": {
          "type": "string",
          "default": "gpt-3.5-turbo",
          "scope": "window",
          "markdownDescription": "The OpenAI engine model",
          "enum": [
            "gpt-3.5-turbo",
            "text-davinci-003",
            "gpt-4",
            "gpt-4-32k"
          ],
          "enumDescriptions": [
            "gpt-3.5-turbo Most capable GPT-3.5 model and optimized for chat at 1/10th the cost of text-davinci-003. 4,096 max tokens",
            "text-davinci-003 Can do any language task with better quality, longer output, and consistent instruction-following than the curie, babbage, or ada models. Also supports inserting completions within text. 4,097 max tokens",
            "gpt-4 More capable than any GPT-3.5 model, able to do more complex tasks, and optimized for chat. 8,192 max tokens",
            "gpt-4-32k Same capabilities as the base gpt-4 mode but with 4x the context length. 32,768 max tokens"
          ]
        }
      }
    },
    "commands": [
      {
        "command": "kmxcoder.analyze",
        "title": "KMX Coder Analyze Selected"
      },
      {
        "command": "kmxcoder.optimize",
        "title": "KMX Coder Optimize Selected"
      },
      {
        "command": "kmxcoder.custom",
        "title": "KMX Coder Custom Command on Selected"
      }
    ]
  },
  "scripts": {
    "vscode:prepublish": "npm run compile",
    "compile": "tsc -p ./",
    "watch": "tsc -watch -p ./",
    "pretest": "npm run compile && npm run lint",
    "lint": "eslint src --ext ts",
    "test": "node ./out/test/runTest.js"
  },
  "devDependencies": {
    "@types/glob": "^8.1.0",
    "@types/mocha": "^10.0.1",
    "@types/node": "16.x",
    "@types/vscode": "^1.77.0",
    "@typescript-eslint/eslint-plugin": "^5.56.0",
    "@typescript-eslint/parser": "^5.56.0",
    "@vscode/test-electron": "^2.3.0",
    "eslint": "^8.36.0",
    "glob": "^8.1.0",
    "mocha": "^10.2.0",
    "typescript": "^4.9.5"
  },
  "dependencies": {
    "axios": "^1.3.6",
    "openai": "^3.2.1"
  }
}
